% Search for all the places that say "PUT SOMETHING HERE".

\documentclass[11pt]{article}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx,enumerate}

\def\Name{Aniket Ketkar, Tanay Lathia, Eric Priest, Steve Wang}  % Your name
\def\SID{24795502}  % Your student ID number
\def\Login{aniket.ketkar@berkeley.edu} % Your login (your class account, cs170-xy)
\def\Homework{3} % Number of Homework
\def\Session{Fall 2015}


\title{Statistics 133 - Final Project - "Team Feelin' the Bern"}
\author{\Name}
\markboth{Stat 133--\Session\  Final Project \Name}{ \Name}
\pagestyle{myheadings}
\date{}

\newenvironment{qparts}{\begin{enumerate}[{(}a{)}]}{\end{enumerate}}
\def\endproofmark{$\Box$}
\newenvironment{proof}{\par{\bf Proof}:}{\endproofmark\smallskip}

\textheight=9in
\textwidth=6.5in
\topmargin=-.75in
\oddsidemargin=0.25in
\evensidemargin=0.25in


\begin{document}
\maketitle

\section*{Abstract}


\section*{Background}

\section*{Questions for Analysis}

\section*{Methods}

\section*{Exploratory Analysis}

\section*{Machine Learning - K Nearest Neighbors}
K Nearest neighbors is a nonparametric machine learning technique to find similar points in a dataset. It makes the assumption that similar points will have similar results, so point's label is predicted as the average label of its neighbors. The downside is the simplicity of the technique and the non-parametric nature; KNN does not make any assumptions about the structure of the dataset, underlying trends, or high level ideas. It just finds the nearest neighbors. For large datasets, like the one in this paper, KNN must run through all of the training points before it can make a prediction. Performance is guaranteed to be linear ($O(n)$) with the 
\section*{KNN Results}

\section*{Final Analysis - Answering the Questions}


\end{document}
